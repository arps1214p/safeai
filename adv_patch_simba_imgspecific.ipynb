{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arps1214p/safeai/blob/main/adv_patch_simba_imgspecific.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5L-81tQCB7Pv",
      "metadata": {
        "id": "5L-81tQCB7Pv"
      },
      "outputs": [],
      "source": [
        "!pip install addict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2dd18ef",
      "metadata": {
        "id": "d2dd18ef"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import math\n",
        "import random\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from skimage import io\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import argparse, yaml\n",
        "from addict import Dict\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"Torchvision Version: \", torchvision.__version__)\n",
        "print(\"Pillow Version: \", PIL.__version__)\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "path = \"/content/checkpoint.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d2bfdd",
      "metadata": {
        "id": "c2d2bfdd"
      },
      "outputs": [],
      "source": [
        "# 모델 구조 불러오기\n",
        "model = models.resnet18(weights=None, num_classes=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18091f18",
      "metadata": {
        "id": "18091f18"
      },
      "outputs": [],
      "source": [
        "# Generalized-Mean Pooling (GeM)\n",
        "# avgpool -> p-norm pooling\n",
        "\n",
        "class GeM(nn.Module):\n",
        "    def __init__(self, p=3.0, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.p = nn.Parameter(torch.ones(1)*p)\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        return F.adaptive_avg_pool2d(x.clamp(min=self.eps).pow(self.p), (1,1)).pow(1./self.p)\n",
        "\n",
        "# 교체\n",
        "model.avgpool = GeM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f463023f",
      "metadata": {
        "id": "f463023f"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load(\n",
        "    path,\n",
        "    map_location=torch.device('cpu'),\n",
        "    weights_only=True\n",
        ")\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f65373",
      "metadata": {
        "id": "25f65373",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7861284f",
      "metadata": {
        "id": "7861284f"
      },
      "source": [
        "### Dataload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8011138",
      "metadata": {
        "id": "b8011138"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"alexattia/the-simpsons-characters-dataset\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c2e363",
      "metadata": {
        "id": "27c2e363"
      },
      "outputs": [],
      "source": [
        "train_dir = Path('/kaggle/input/the-simpsons-characters-dataset/simpsons_dataset/simpsons_dataset')\n",
        "test_dir = Path('/kaggle/input/the-simpsons-characters-dataset/kaggle_simpson_testset/kaggle_simpson_testset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f798897f",
      "metadata": {
        "id": "f798897f"
      },
      "outputs": [],
      "source": [
        "class TrainValTestSplit():\n",
        "\n",
        "  def __init__(self, train_dir, test_dir):\n",
        "\n",
        "    self.train_dir = train_dir\n",
        "    self.test_dir = test_dir\n",
        "    # 하위 디렉토리를 순회하며 이미지의 경로를 리스트로 저장\n",
        "    self.train_val_files_path = sorted(list(self.train_dir.rglob('*.jpg')))\n",
        "    self.test_path = sorted(list(self.test_dir.rglob('*.jpg')))\n",
        "    self.train_val_labels = [path.parent.name for path in self.train_val_files_path]\n",
        "\n",
        "  def get_path(self):\n",
        "\n",
        "    train_files_path, val_files_path = train_test_split(self.train_val_files_path, test_size = 0.3, \\\n",
        "                                          stratify=self.train_val_labels, random_state = 42)\n",
        "\n",
        "    train_val_files_path = {'train': train_files_path, 'val': val_files_path}\n",
        "\n",
        "    return train_val_files_path, self.test_path\n",
        "\n",
        "  def get_n_classes(self):\n",
        "    return len(np.unique(self.train_val_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0f8349",
      "metadata": {
        "id": "5f0f8349"
      },
      "outputs": [],
      "source": [
        "TrainValTestPath = TrainValTestSplit(train_dir, test_dir)\n",
        "train_path, test_path = TrainValTestPath.get_path()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b26354",
      "metadata": {
        "id": "d5b26354"
      },
      "outputs": [],
      "source": [
        "# ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) -> imagenet 데이터셋의 통계 기반\n",
        "input_size = 224\n",
        "\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((input_size,input_size)),\n",
        "        #transforms.CenterCrop(input_size),\n",
        "        transforms.RandomChoice( [\n",
        "                                  transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                  transforms.ColorJitter(contrast=0.5),\n",
        "                                  transforms.ColorJitter(brightness=0.1),\n",
        "                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(contrast=0.5) ], p=0.5),\n",
        "                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(brightness=0.1) ], p=0.5),\n",
        "                                  ] ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((input_size,input_size)),\n",
        "        #transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265cc15c",
      "metadata": {
        "id": "265cc15c"
      },
      "outputs": [],
      "source": [
        "class SimpsonsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, files_path, data_transforms):\n",
        "      self.files_path = files_path\n",
        "      self.transform = data_transforms\n",
        "\n",
        "      if 'test' not in str(self.files_path[0]):\n",
        "        self.labels = [path.parent.name for path in self.files_path]\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(self.labels)\n",
        "\n",
        "        with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
        "            pickle.dump(self.label_encoder, le_dump_file)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.files_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      img_path = str(self.files_path[idx])\n",
        "      image = Image.open(img_path)\n",
        "      image = self.transform(image)\n",
        "\n",
        "      if 'test' in str(self.files_path[0]):\n",
        "        return image\n",
        "      else:\n",
        "        label_str = str(self.files_path[idx].parent.name)\n",
        "        label = self.label_encoder.transform([label_str]).item()\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbad2bd",
      "metadata": {
        "id": "5fbad2bd"
      },
      "outputs": [],
      "source": [
        "image_datasets = {mode: SimpsonsDataset(train_path[mode], data_transforms[mode]) for mode in ['train', 'val']}\n",
        "image_datasets_test = SimpsonsDataset(test_path, data_transforms['val'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61a1bfb",
      "metadata": {
        "id": "c61a1bfb"
      },
      "outputs": [],
      "source": [
        "wordker_id = 42\n",
        "num_workers = 0\n",
        "batch_size = 1\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n",
        "\n",
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True,\n",
        "                                                         num_workers=num_workers, worker_init_fn=seed_worker,generator=g),\n",
        "                    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True,\n",
        "                                                       num_workers=num_workers,worker_init_fn=seed_worker,generator=g)}\n",
        "dataloader_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=batch_size, shuffle=False,\n",
        "                                              num_workers=num_workers, worker_init_fn=seed_worker,generator=g)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "score based blackbox attack -> 모델의 출력 logit을 알 수 있는 상황"
      ],
      "metadata": {
        "id": "d2Iect54SFZx"
      },
      "id": "d2Iect54SFZx"
    },
    {
      "cell_type": "code",
      "source": [
        "def simba_attack_targeted_patch(model, image, true_label, target_label,\n",
        "                                 epsilon=0.2, max_iters=1000, patch_size=50):\n",
        "    \"\"\"\n",
        "    SimBA Targeted Attack (Only within a square patch)\n",
        "\n",
        "    Args:\n",
        "        model:         classification model\n",
        "        image:         input tensor (1, C, H, W)\n",
        "        true_label:    true label (tensor of shape [1])\n",
        "        target_label:  target class (tensor of shape [1])\n",
        "        epsilon:       step size\n",
        "        max_iters:     maximum steps\n",
        "        patch_size:    side length of square patch (e.g., 50 for 50×50 region)\n",
        "\n",
        "    Returns:\n",
        "        adversarial image, number of model queries\n",
        "    \"\"\"\n",
        "    image = image.clone().detach()\n",
        "    perturbed = image.clone().detach()\n",
        "    device = next(model.parameters()).device\n",
        "    image = image.to(device)\n",
        "    perturbed = perturbed.to(device)\n",
        "    true_label = true_label.to(device)\n",
        "    target_label = target_label.to(device)\n",
        "    succeeded = 0\n",
        "\n",
        "    c, h, w = image.shape[1:]\n",
        "    query_count = 0\n",
        "\n",
        "\n",
        "    # 오른쪽 아래 구석에 패치 적용\n",
        "    # starting pixel을 조정하여 패치 적용 위치 조절\n",
        "    patch_h = min(patch_size, h)\n",
        "    patch_w = min(patch_size, w)\n",
        "    starting_h = h - patch_h\n",
        "    starting_w = w - patch_w\n",
        "\n",
        "    # Define valid pixel indices inside the patch only\n",
        "    indices = [(ch, i, j) for ch in range(c)\n",
        "                         for i in range(starting_h, starting_h + patch_h)\n",
        "                         for j in range(starting_w, starting_w + patch_w)]\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        orig_pred = output.argmax(dim=1)\n",
        "\n",
        "    if orig_pred.item() == target_label.item():\n",
        "        print(\"이미 target class로 분류되고 있습니다.\")\n",
        "        succeeded = 1\n",
        "        return perturbed, query_count, succeeded\n",
        "\n",
        "    for i in tqdm_notebook(range(min(max_iters, len(indices))), desc=\"SimBA Targeted Patch Attack\"):\n",
        "        ch, row, col = indices[i]\n",
        "\n",
        "        direction = torch.zeros_like(image).to(device)\n",
        "        direction[0, ch, row, col] = epsilon\n",
        "\n",
        "        logits1 = model(perturbed + direction)\n",
        "        logits2 = model(perturbed - direction)\n",
        "        query_count += 2\n",
        "\n",
        "        loss1 = criterion(logits1, target_label)\n",
        "        loss2 = criterion(logits2, target_label)\n",
        "\n",
        "        if loss1 < loss2:\n",
        "            perturbed = perturbed + direction\n",
        "        else:\n",
        "            perturbed = perturbed - direction\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(perturbed).argmax(dim=1)\n",
        "        if pred.item() == target_label.item():\n",
        "            print(f\"공격 성공! step {i+1}, queries: {query_count}\")\n",
        "            succeeded = 1\n",
        "            break\n",
        "    return perturbed.detach(), query_count, succeeded, (starting_h, starting_w, patch_h, patch_w)"
      ],
      "metadata": {
        "id": "0H7ydZMpCXk-"
      },
      "id": "0H7ydZMpCXk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images_and_return_perturbation(original, adversarial, title_prefix=\"\"):\n",
        "      \"\"\"\n",
        "      시각화: 원본, adversarial, perturbation\n",
        "      \"\"\"\n",
        "      # CPU로 이동 및 Tensor → NumPy\n",
        "      orig_np = original.squeeze().detach().cpu().permute(1, 2, 0).numpy()\n",
        "      adv_np = adversarial.squeeze().detach().cpu().permute(1, 2, 0).numpy()\n",
        "      perturbation = adv_np - orig_np\n",
        "\n",
        "      # 시각화\n",
        "      fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "      axs[0].imshow(np.clip(orig_np, 0, 1))\n",
        "      axs[0].set_title(f\"{title_prefix}Original\")\n",
        "      axs[0].axis(\"off\")\n",
        "\n",
        "      axs[1].imshow(np.clip(adv_np, 0, 1))\n",
        "      axs[1].set_title(f\"{title_prefix}Adversarial\")\n",
        "      axs[1].axis(\"off\")\n",
        "\n",
        "      # 퍼터베이션 강조 시각화 (±범위 기준 정규화)\n",
        "      pert = perturbation / (np.max(np.abs(perturbation)) + 1e-8) / 2 + 0.5\n",
        "      axs[2].imshow(np.clip(pert, 0, 1))\n",
        "      axs[2].set_title(f\"{title_prefix}Perturbation (scaled)\")\n",
        "      axs[2].axis(\"off\")\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      return perturbation\n",
        "\n",
        "os.makedirs(\"/kaggle/adv_images\", exist_ok=True)\n",
        "os.makedirs(\"/kaggle/adv_patches\", exist_ok=True)\n",
        "i = 0\n",
        "for img, true_lbl in dataloaders_dict['val']:\n",
        "    # Target label: any class different from true label\n",
        "    target_lbl = torch.tensor([25]) # 25 = milhouse\n",
        "    print(true_lbl)\n",
        "\n",
        "    img = img.to(device)\n",
        "    true_lbl = true_lbl.to(device)\n",
        "    target_lbl = target_lbl.to(device)\n",
        "\n",
        "    adv_img, queries, succeeded, patch_info  = simba_attack_targeted_patch(\n",
        "        model, img, true_lbl, target_lbl,\n",
        "        epsilon=3, max_iters=50000\n",
        "    )\n",
        "\n",
        "    i+=1\n",
        "\n",
        "    perturbation = show_images_and_return_perturbation(img, adv_img, title_prefix=\"SimBA - \")\n",
        "\n",
        "    if succeeded:\n",
        "      patch_file_name = f\"adv_patch{i}.jpg\"\n",
        "      patch_image_name = f\"adv_image{i}.jpg\"\n",
        "\n",
        "      patch_file_path = os.path.join(\"/kaggle/adv_patches\", patch_file_name)\n",
        "      patch_image_path = os.path.join(\"/kaggle/adv_images\", patch_image_name)\n",
        "\n",
        "      starting_h, starting_w, patch_h, patch_w = patch_info\n",
        "\n",
        "      perturbation_patch = perturbation[starting_h : starting_h + patch_h,\n",
        "                                                   starting_w : starting_w + patch_w, :]\n",
        "\n",
        "      unnormalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255], std=[1/0.229, 1/0.224, 1/0.255])\n",
        "      adv_patch_unnormalized = unnormalize(torch.Tensor(perturbation_patch).permute(2,0,1)) # Remove batch dimension\n",
        "      adv_image_unnormalized = unnormalize(adv_img.squeeze(0))\n",
        "\n",
        "      torchvision.utils.save_image(adv_patch_unnormalized, patch_file_path)\n",
        "      torchvision.utils.save_image(adv_image_unnormalized, patch_image_path)\n",
        "\n",
        "    # 10개만 생성\n",
        "    if i == 10:\n",
        "      break"
      ],
      "metadata": {
        "id": "bIyuv1i_Clpv"
      },
      "id": "bIyuv1i_Clpv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}